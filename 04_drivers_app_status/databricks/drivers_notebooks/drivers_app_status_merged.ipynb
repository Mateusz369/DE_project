{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Drivers Application Status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airflow DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.providers.standard.operators.python import PythonOperator, BranchPythonOperator\n",
    "from airflow.providers.databricks.operators.databricks import DatabricksSubmitRunOperator\n",
    "from airflow.providers.standard.operators.empty import EmptyOperator\n",
    "\n",
    "from pendulum import duration\n",
    "from datetime import datetime, timezone\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the date equals today by looking at the lastupdate field in the API response. If it's up to date, return true, otherwise return false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_today() -> bool:\n",
    "    url = \"https://data.cityofnewyork.us/resource/dpec-ucu7.json?$limit=20&$order=lastupdate DESC\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=20)\n",
    "        data = response.json()\n",
    "        today = datetime.now(timezone.utc).strftime('%Y-%m-%d')\n",
    "        \n",
    "        for row in data:\n",
    "            lastupdate = row.get('lastupdate', '')\n",
    "            if lastupdate and lastupdate.startswith(today):\n",
    "                return True\n",
    "        return False\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return 'download' or 'skip' based on the is_today() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide(**context) -> str:\n",
    "    return 'download' if is_today() else 'skip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data from the API and save it as a CSV file in the local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file_name: str) -> None:\n",
    "    import csv\n",
    "\n",
    "    try:\n",
    "        url = \"https://data.cityofnewyork.us/resource/dpec-ucu7.json?$limit=10000\"\n",
    "\n",
    "        response = requests.get(url, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        if not data:\n",
    "            raise RuntimeError(\"No data received from the API\")\n",
    "\n",
    "        keys = data[0].keys()\n",
    "\n",
    "        with open(file_name, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=keys)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(data)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to get data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect with Databricks environement and upload the CSV file from the local directory to Databricks volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_volume(**context):\n",
    "    from airflow.sdk.bases.hook import BaseHook\n",
    "    from databricks.sdk import WorkspaceClient\n",
    "\n",
    "    ds = context['ds']\n",
    "    file_name = f\"tlc_driver_application_{ds}.csv\"\n",
    "\n",
    "    local_file = f\"/airflow/{file_name}\"\n",
    "    volume_file = f\"/Volumes/driver_app_status/raw_data/raw_data/{file_name}\"\n",
    "\n",
    "    conn = BaseHook.get_connection('databricks_ingestion')\n",
    "    host = conn.host.rstrip('/')\n",
    "    token = conn.password or conn.extra_dejson.get('token')\n",
    "    w = WorkspaceClient(host=host, token=token)\n",
    "\n",
    "    with open(local_file, \"rb\") as f:\n",
    "        w.files.upload(volume_file, f, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_args = {\n",
    "    \"owner\": \"airflow\",\n",
    "    \"depends_on_past\": False,\n",
    "    \"email_on_failure\": False,\n",
    "    \"email_on_retry\": False,\n",
    "    \"retries\": 2,\n",
    "    \"retry_delay\": duration(minutes=2),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the DAG with the specified parameters and tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with DAG(\n",
    "    dag_id=\"drivers_status_dag\",\n",
    "    start_date=datetime(2026, 1, 15),\n",
    "    #end_date=datetime(2026, 2, 15),\n",
    "    schedule=\"0 20 * * *\",\n",
    "    catchup=False,\n",
    "    default_args=default_args,\n",
    "    tags=[\"databricks\", \"drivers\", \"daily\"],\n",
    ") as dag:\n",
    "\n",
    "\n",
    "    check_today = BranchPythonOperator(\n",
    "        task_id='check_today',\n",
    "        python_callable=decide,\n",
    "    )\n",
    "\n",
    "\n",
    "    download = PythonOperator(\n",
    "        task_id=\"download\",\n",
    "        python_callable=get_data,\n",
    "        op_kwargs={\"file_name\": \"/airflow/tlc_driver_application_{{ds}}.csv\"},\n",
    "       \n",
    "    )\n",
    "\n",
    "    ingest_csv = PythonOperator(\n",
    "        task_id='ingest_csv',\n",
    "        python_callable=upload_to_volume,\n",
    "    )\n",
    "\n",
    "    skip = EmptyOperator(task_id='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    check_today >> [download, skip]\n",
    "    download >> ingest_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Airflow](images/air01.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e216019-8a59-47eb-ad20-c98f66cdc74f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1. Setting up project environment\n",
    "Creating catalog, schemas, volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "108b76d3-7b06-47c9-825a-5a313d404c6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 2
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE CATALOG driver_app_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e660e510-4fb5-4d15-ae19-4d9523201c46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "'raw_data' will be used for uploaded .csv files, 'dlt_schema' for all other data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55976e22-e006-40db-8261-c369f974309c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 8
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE SCHEMA driver_app_status.raw_data;\n",
    "\n",
    "CREATE SCHEMA driver_app_status.dlt_schema;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "826a2cea-974c-4180-850e-177ed4e5370e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 12
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "USE CATALOG driver_app_status;\n",
    "USE SCHEMA raw_data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab69ed06-4754-4b9a-8570-b4ed141b87a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 13
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE VOLUME raw_data\n",
    "  COMMENT 'CSV raw file';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bca6f663-3562-4f09-92e2-123fc658acef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2. Defining input\n",
    "Saving data to delta table and adding a widget for current date (will be set up dynamically in job setting later) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86f9d7d5-fa97-4c5f-a857-b7724d55da15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\n",
    "    name=\"file_date\",\n",
    "    defaultValue=\"\"\n",
    ")\n",
    "\n",
    "file_date = dbutils.widgets.get(\"file_date\")\n",
    "\n",
    "if not file_date:\n",
    "    from datetime import date\n",
    "    file_date = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "file_path = f\"/Volumes/driver_app_status/raw_data/raw_data/tlc_driver_application_{file_date}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "290856fd-0509-48f1-9584-34c4edde949e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = (spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(file_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8913442b-7fa6-4264-a70e-c2883ce6a8e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write \\\n",
    "  .format(\"delta\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .option(\"overwriteSchema\", \"true\") \\\n",
    "  .saveAsTable(\"driver_app_status.dlt_schema.raw_drivers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Ingestion bronze\n",
    "Defining exception rules for bronze layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "exception_rules ={\n",
    "    \"app_no\" : \"app_no IS NOT NULL\",\n",
    "    \"lastupdate\" : \"lastupdate IS NOT NULL\",\n",
    "    \"status\" : \"status IS NOT NULL\",\n",
    "}\n",
    "\n",
    "columns = [\"app_date\", \"app_no\", \"type\", \"status\", \"lastupdate\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating bronze table - raw data with basic quality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "  name=\"drivers_bronze\"\n",
    ")\n",
    "\n",
    "@dlt.expect_all_or_drop(exception_rules)\n",
    "\n",
    "\n",
    "def drivers_bronze():\n",
    "    df=spark.readStream.table(\"driver_app_status.dlt_schema.raw_drivers\")\n",
    "    df = df.select(columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Transform drivers\n",
    "Creating silver view for cleaned and transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "@dlt.view(\n",
    "    name=\"drivers_silver_view\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic silver transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drivers_silver_view():\n",
    "    df = spark.readStream.table(\"drivers_bronze\")\n",
    "    df = df.withColumn(\"app_date\", to_date(col(\"app_date\"), \"yyyy-MM-dd\"))\n",
    "    df = df.withColumn(\"lastupdate\", to_date(col(\"lastupdate\"), \"yyyy-MM-dd\"))\n",
    "    df = df.filter(col(\"app_date\") >= '2025-01-01')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating silver streaming table (SCD Type 1) - keeps only the latest record per 'app_no'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlt.create_streaming_table(\n",
    "    name = \"drivers_silver\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying auto CDC flow for upserts based on 'lastupdate' (SCD Type 1 - overwrite with latest record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlt.create_auto_cdc_flow(\n",
    "    target = \"drivers_silver\", \n",
    "    source = \"drivers_silver_view\",\n",
    "    keys = [\"app_no\"],\n",
    "    sequence_by = \"lastupdate\",\n",
    "    ignore_null_updates=False,\n",
    "    apply_as_deletes=None,\n",
    "    apply_as_truncates=None,\n",
    "    column_list=None,\n",
    "    except_column_list=None,\n",
    "    stored_as_scd_type = 1, \n",
    "    track_history_column_list=None,\n",
    "    track_history_except_column_list=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Creating gold streaming table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "\n",
    "dlt.create_streaming_table(\n",
    "    name = \"dim_drivers\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the flow from silver view and storing as SCD type 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlt.create_auto_cdc_flow(\n",
    "    target = \"dim_drivers\",\n",
    "    source = \"drivers_silver_view\",\n",
    "    keys = [\"app_no\"],\n",
    "    sequence_by = \"lastupdate\",\n",
    "    ignore_null_updates=False,\n",
    "    apply_as_deletes=None,\n",
    "    apply_as_truncates=None,\n",
    "    column_list=None,\n",
    "    except_column_list=None,\n",
    "    stored_as_scd_type = 2,\n",
    "    track_history_column_list=None,\n",
    "    track_history_except_column_list=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2dc18374-44e4-4f7a-94d1-b7286a4c29bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 6. Final results \n",
    "Creating a view for Slowly Changing Dimension Type 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ae18425-2906-4277-82db-5bc3a9119689",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.databricks.empty-table+json": {
       "directive_name": "CreateView"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE VIEW IF NOT EXISTS driver_app_status.dlt_schema.drivers_scd2 AS\n",
    "SELECT app_date,\n",
    "  app_no,\n",
    "\ttype,\n",
    "  status,\n",
    "\tMIN(lastupdate) AS start_date,\n",
    "\tMAX(lastupdate) AS end_date\n",
    "FROM driver_app_status.dlt_schema.dim_drivers\n",
    "--WHERE app_no = '6129488'\n",
    "--WHERE app_no = '6117523'\n",
    "GROUP BY app_date, app_no, type, status\n",
    "ORDER BY app_no, start_date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5dc5ce48-87e2-46ce-a311-ada74e224f65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Based on the created view, we can see the final SCD type 2 table, which tracks the changes accross dimensions (app_date, app_no and type are not changing). It indicates the current status of a driver application in a defined period of time and holds much less data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09191841-849e-4963-ad3c-1ee569ff5191",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>app_date</th><th>app_no</th><th>type</th><th>status</th><th>start_date</th><th>end_date</th></tr></thead><tbody><tr><td>2026-01-30</td><td>6131556</td><td>HDR</td><td>Incomplete</td><td>2026-02-02</td><td>2026-02-04</td></tr><tr><td>2026-01-30</td><td>6131556</td><td>HDR</td><td>Under Review</td><td>2026-02-05</td><td>2026-02-09</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2026-01-30",
         6131556,
         "HDR",
         "Incomplete",
         "2026-02-02",
         "2026-02-04"
        ],
        [
         "2026-01-30",
         6131556,
         "HDR",
         "Under Review",
         "2026-02-05",
         "2026-02-09"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "app_date",
            "nullable": true,
            "type": "date"
           },
           {
            "metadata": {},
            "name": "app_no",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "type",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "status",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "start_date",
            "nullable": true,
            "type": "date"
           },
           {
            "metadata": {},
            "name": "end_date",
            "nullable": true,
            "type": "date"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 3
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "app_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "app_no",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "start_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "end_date",
         "type": "\"date\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM driver_app_status.dlt_schema.drivers_scd2\n",
    "WHERE app_no = '6131556'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfe0246c-5908-4ca0-af49-b62c45ff7dce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Below is a default Databricks SCD type 2 format based on gold layer with generated _START_AT and _END_AT columns. It holds a lot more data and treats given end date as a start date of a new day, which makes it is less desirable for both reasons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fafccc52-a812-4086-973f-d2a2415a2044",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>app_date</th><th>app_no</th><th>type</th><th>status</th><th>lastupdate</th><th>__START_AT</th><th>__END_AT</th></tr></thead><tbody><tr><td>2026-01-30</td><td>6131556</td><td>HDR</td><td>Incomplete</td><td>2026-02-02</td><td>2026-02-02</td><td>2026-02-03</td></tr><tr><td>2026-01-30</td><td>6131556</td><td>HDR</td><td>Incomplete</td><td>2026-02-03</td><td>2026-02-03</td><td>2026-02-04</td></tr><tr><td>2026-01-30</td><td>6131556</td><td>HDR</td><td>Incomplete</td><td>2026-02-04</td><td>2026-02-04</td><td>2026-02-05</td></tr><tr><td>2026-01-30</td><td>6131556</td><td>HDR</td><td>Under Review</td><td>2026-02-05</td><td>2026-02-05</td><td>2026-02-06</td></tr><tr><td>2026-01-30</td><td>6131556</td><td>HDR</td><td>Under Review</td><td>2026-02-06</td><td>2026-02-06</td><td>2026-02-07</td></tr><tr><td>2026-01-30</td><td>6131556</td><td>HDR</td><td>Under Review</td><td>2026-02-07</td><td>2026-02-07</td><td>2026-02-08</td></tr><tr><td>2026-01-30</td><td>6131556</td><td>HDR</td><td>Under Review</td><td>2026-02-08</td><td>2026-02-08</td><td>2026-02-09</td></tr><tr><td>2026-01-30</td><td>6131556</td><td>HDR</td><td>Under Review</td><td>2026-02-09</td><td>2026-02-09</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2026-01-30",
         6131556,
         "HDR",
         "Incomplete",
         "2026-02-02",
         "2026-02-02",
         "2026-02-03"
        ],
        [
         "2026-01-30",
         6131556,
         "HDR",
         "Incomplete",
         "2026-02-03",
         "2026-02-03",
         "2026-02-04"
        ],
        [
         "2026-01-30",
         6131556,
         "HDR",
         "Incomplete",
         "2026-02-04",
         "2026-02-04",
         "2026-02-05"
        ],
        [
         "2026-01-30",
         6131556,
         "HDR",
         "Under Review",
         "2026-02-05",
         "2026-02-05",
         "2026-02-06"
        ],
        [
         "2026-01-30",
         6131556,
         "HDR",
         "Under Review",
         "2026-02-06",
         "2026-02-06",
         "2026-02-07"
        ],
        [
         "2026-01-30",
         6131556,
         "HDR",
         "Under Review",
         "2026-02-07",
         "2026-02-07",
         "2026-02-08"
        ],
        [
         "2026-01-30",
         6131556,
         "HDR",
         "Under Review",
         "2026-02-08",
         "2026-02-08",
         "2026-02-09"
        ],
        [
         "2026-01-30",
         6131556,
         "HDR",
         "Under Review",
         "2026-02-09",
         "2026-02-09",
         null
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "app_date",
            "nullable": true,
            "type": "date"
           },
           {
            "metadata": {},
            "name": "app_no",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {},
            "name": "type",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "status",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "lastupdate",
            "nullable": true,
            "type": "date"
           },
           {
            "metadata": {},
            "name": "__START_AT",
            "nullable": true,
            "type": "date"
           },
           {
            "metadata": {},
            "name": "__END_AT",
            "nullable": true,
            "type": "date"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 6
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "app_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "app_no",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "lastupdate",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "__START_AT",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "__END_AT",
         "type": "\"date\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM driver_app_status.dlt_schema.dim_drivers\n",
    "WHERE app_no = '6131556'\n",
    "ORDER BY lastupdate"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6860328247276549,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_driver_app_status",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
